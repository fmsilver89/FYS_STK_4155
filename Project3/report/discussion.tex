\section{Discussion}
\setlength{\parindent}{0cm}
The  methods of logistic regression and linear SVM produce similar results, the logistic regression seems to be more sensitive to the regularization parameter $C$. This could most likely be explained by the difference in loss functions, where the logistic 'cross entropy function' diverges faster than the 'hinge loss function' used in linear SVM.
\\
\par
A quick look at the Kernel SVM results implies that the transformation of data by the kernel trick, yields substantially better results. Especially the training results are better, which imply that the model is also overfitting to a larger extent. The trade-off in accuracy for test data is regardless favourable in this case. When it comes to the neural network performance, we did not expect the results to be greater than the SVM, since our data set is not considered large. Neural network have well-known learning limitations when it comes to small datasets. However, the method produced satisfying results in line with kernel SVM. The drawback in comparison is the computational time. What SVM was able to produce in seconds, neural network produced in several minutes, preforming the same computations without the packages would most likely scale the differences even further.
\\
\par
An interesting aspect of the results is the difference between the 2012 and 2016 election. In this problem, one would expect 2012 training data to fit 2012 test data better, which is not the case. An explanation could be the accuracy paradox, where the target is even more unbalanced for the 2016 election, making the overall classification prone to higher accuracy score. 
\\
\par
There's a spatial dependence that be accounted for to further enhance the model. The spatial dependence is that the locations closer to each other are more likely to have similar values than those that are further away. In this case, counties closer to each other are more likely to have similar class number or label. This phenomenon is known as Tobler's first law of geography. A suitable distance or neighbour weighting could implemented to take advantage of this. One approach is the Anselin Local Moran's I autocorrelation index. 
\\
\par
Based on the variable performance, all variables included in this dataset contributes with valuable information. A variable selection test was made, and the result confirmed the statement above as it yielded worse results for selected variables, see the appendix [1]. A quick eye on the variables in figure [12], implies that the size and age of the population matters the most. This corresponds well with the reality that the most dense cities generally linked to high democratic support, as can be seen in figure [15].
\\
\par
As for the flipped case with 257 observations, the SVM algorithm was preferred since it is better suited for small datasets. The set for the flipped case was also heavily unbalanced, as the republicans had 95\% of the flips. The combination of a small data set and unbalanced data was handled to some extent by cross validation and the balance function included in Scikit learn. However, the accuracy score varied with the percentage of democrats included in the test data. Another metric approach was tried, where we used the F1 score metric. The results were significantly different, as can be seen in figure[14]. This further highlights the accuracy paradox.